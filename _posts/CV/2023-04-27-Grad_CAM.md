---

title:  "[Paper Review] Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"

excerpt: "Grad-CAM 논문 리뷰"
categories:
  - CV
toc_label: "  목차"
toc: true
toc_sticky: true

date: 2023-04-27
last_modified_at: 2023-04-27
---
## Background

CNN 기반의 deep neural model은 image classification, object detection 등 다양한 computer vision task에서 훌륭한 성능을 보였다. 그러나 각각의 직관적인 요소로의 decomposability 부족은 이들을 해석하기 어렵게 만들었고, 이는 사용자가 지능형 시스템의 일관성 없는 output을 보면서 시스템이 왜 그런 의사결정을 했는지에 대해서 궁금하게 한다. Imterpretability matters. 지능형 시스템에 있어서 신뢰를 구축하기 위해서는 왜 그렇게 예측했는지를 설명할 능력을 가진 ‘transparent’ model을 구축할 필요가 있다.

Zhou et al. 은 discriminative regions을 식별하고자 fully-connected layer가 없는 image classification CNN에 사용되는 기법인 Class Activation Map을 제안하였다. 이는 fully-commected layer을 제거하므로 성능이 줄어들게 되는 단점이 있었다. 그에 반해 본 논문의 저자들은 모델의 architecture을 바꾸지 않고 존재하는 SOTA deep model을 interpretable하게 만들며 이를 통하여 모델의 복잡성과 성능 사이의 trade-off를 회피한다. 본 논문에서 제시하는 접근법은 CAM의 generalization이며, 다양한 CNN 기반의 model에 적용 가능하다. 

## Grad-CAM

Grad-CAM은 CNN의 마지막 convolutional layer로 흐르는 gradient information을 사용하여 관심이 있는 특정한 의사결정을 위해 각 뉴런에 importance value를 할당하는 방법이다. Figure1에서 볼 수 있듯, width가 $u$이고 height가 $v$인 어떤 class $c$에 대한 class-discriminative localization map Grad-CAM $L^c_{Grad−CAM} \in ****R^{u×v}$를 얻기 위해서 저자들은 먼저  class $c$에 대한 점수 $y^c$(before the softmax)에 대한 gradient를 convoultional layer의 feature map activation $A^k$에 대해서 계산한다.  이는 {% raw %}${{\partial y^c}\over{\partial A^k}}{% endraw %}로 표현할 수 있다. 이 gradients는 nueron importance weights $\alpha _k^c$를 얻기 위해서 width와 height의 dimension에 대해서 global average pooled 된다. 이렇게 얻은 weights $\alpha _k^c$는 target class $c$에 대한 feature map $k$의 ‘importance’를 포착한다.

저자들은 다음으로 저자들은 forward activation maps의 weighted combination과 ReLU를 수행하여 convolutional feature maps와 동일한 사이즈의 coarse heatmap의 결과를 생성한다. 이를 식으로 나타내면 아래와 같다. 

{% raw %}
$$
\alpha _k^c = {1 \over Z} \sum_i \sum_j { {\partial y^c}\over{\partial A^k}}
$$
{% endraw %}

{% raw %}
$$ 
L^c_{Grad−CAM} = ReLU(\sum_k \alpha _k^c A^k) 
$$
{% endraw %}

저자들은 maps의 linear combination에 ReLU를 적용하였는데, 이는 저자들이 오직 관심 있는 class에 positive 영향을 주는 feature에만 관심이 있기 때문이다. $y^c$는 image classification CNN에서 만들어진 class score일 필요는 없으며, caption이나 question에 대한 answer로부터 나오는 word를 포함한 어떠한 미분 가능한 activation도 모두 적용 가능하다.

### Grad-CAM generalizes CAM

Grad-CAM이 다양한 CNN-based architecture에 대해 CAM을 일반화한다는 것을 형식적으로 검증해보도록 하겠다. $A^k_{i,j}$를 feature map A^k의 location $(i,j)$에 있는 activation을 의미한다고 했을 때,  $A^k_{i,j}$는 Global Average Pooling (GAP)을 사용하여 공간적으로 pooling 되며 각 class $c$에 대한 score $Y^c$을 만들어내고자 선형적으로 변형된다. 

{% raw %}
$$
Y^c=\sum_k w_k^c {1\over Z}\sum_i \sum_j A^k_{ij}
$$
{% endraw %}

$F^k$를 Global Average Pooled output이라 가정하면 위 식은 아래와 같이 바꿔 쓸 수 있다. 

{% raw %}
$$
F^k = {1\over Z} \sum_i \sum_j A^k_{ij} 
$$
{% endraw %}

{% raw %}
$$
Y^c = \sum_k w^c_k \cdot F^k
$$
{% endraw %}

위의 식들을 사용하면 class $c$에 대한 score $Y^c$의 feature map $F^k$에 대한 gradient는 아래와 같이 계산된다. 

{% raw %}
$$
{{\partial Y^c} \over {\partial F^k}} = 
{{{\partial Y^c} \over {\partial A^k_{ij}}} \over {{\partial F^k} \over {\partial A^k_{ij}}}} = 
{{\partial Y^c} \over {\partial A^k_{ij}}} \cdot Z = 
w^c_k
$$
{% endraw %}

위 식의 양변을 모든 픽셀 $(i,j)$에 대해 더하면 아래와 같이 정리 가능하다. 

{% raw %}
$$
w^c_k = \sum_i \sum_j {{\partial Y^c} \over {\partial A^k_{ij}}}
$$
{% endraw %}

위 식은 normalize out 해주는 proportionality constant ($1/Z$)를 빼면, $w^c_k $라는 expression은 Grad-CAM에 의해서 사용되는 $\alpha_c^k$와 동일하다는 것을 알 수 있다. 따라서, Grad-CAM은 CAM의 엄격한 generalization이다.

### Guided Grad-CAM

<p style="text-align: center;">
  <img src="/images/grad_1.png" width="60%">
</p>

Grad-CAM은 class-discriminative 하고 관련이 있는 image region의 위치를 찾아주지만, 이는 Guided Backpropagation이나 Deconvolution과 같은 pixel-space gradient visualization methods와 같이 fine-grained details를 강조하는 능력은 다소 부족하다. 

Figure (c)를 보게 되면, Grad-CAM은 쉽게 고양이의 위치를 찾아내지만, coarse heatmap으로부터 왜 network가 이 특정한 instance를 'tiger cat'으로 예측했는지는 불명확하다. 양쪽의 장점을 결합하고자, 저자들은 Guided Backpropagation과 Grad-CAM visualizations을 element-wise multiplication을 통해서 융합한다. 이를 통해 얻게 되는 visualization은 high-resolution이고 class-discriminative 하다.

### Counterfactual Explanations

Grad-CAM을 약간 수정하면 네트워크가 이것의 예측을 바꾸게 만드는 지역을 강조하는 설명을 얻을 수 있게 된다.그 결과로, 이러한 regions에서 나타나는 concept을 제거함으로써 예측에 대한 신뢰성을 높일 수 있고 이러한 explanation modality를 counterfactual explanations이라고 부른다. 

구체적으로 저자들은 class $c$에 대한 score인 $y^c$의 convolutional layer의 feature maps $A$에 대한 gradient 값을 마이너스로 만든다. 따라서 $\alpha_c^k$는 아래와 같은 식으로 바뀌게 되고, 이를 forward activation maps $A$의 weighted sum을 취한 후 ReLU에 통과시켜 counterfactual explanations을 얻게 된다. 결과는 Figure3과 같은 counterfactual explanations을 얻게 된다. 

{% raw %}
$$
\alpha^c_k = {1\over Z}\sum_i\sum_j - { {\partial y^c}\over{\partial A^k_{ij}} }
$$
{% endraw %}

## Evaluating Localization Ability of Grad-CAM

<p style="text-align: center;">
  <img src="/images/grad_2.png" width="50%">
</p>

Image classification 영역에서 Grad-CAM의 localization 능력을 평가해보도록 하겠다. 

CAM 논문에서와 동일하게, 이미지가 주어졌을 때 network는 class prediction을 하게 되며 만들어진 Grad-CAM map의 max 값의 15$\%$를 threshold로 지정하여 이보다 큰 값들을 가지게 되는 map의 위치들을 포함할 수 있는 하나의 bounding box를 만들어 평가한다. 

ILSVRC-15 데이터셋에 대해서 localization error는 Figure4와 같다. VGG16에 대한 Grad-CAM이 top-1 loalization error에서 최고 성능을 나타냈으며, CAM은 모델 구조의 변경으로 인해 re-training이 필요하고 classification error가 높아지지만, Grad-CAM은 classification performance에 있어서 악화되는 현상이 없다는 것이 장점이다.

## Evaluating Visualizations

실험은 90개의 image-category pair에 대해서 4가지의 visualization (Deconvolution, Guided Backpropagation, Deconvolution Grad-CAM, Guided Grad-CAM)을 제시하고, 각 이미지에 대해서 정답이 무엇인지에 대한 평가를 받는 human study이다. Guided Grad-CAM을 보여줬을 때, 실험에 참가한 사람들은 케이스의 61.23$\%$에 대해서 category를 맞췄으며, 이는 Guided Backpropagation의 44.44$\%$와 비교했을 때 human performance를 16.79$\%$만큼 향상시킨 결과이다. 유사하게, Grad-CAM은 Deconvolution을 더욱 class-discriminative 하게 만들었으며, 53.33$\%$에서 60.37$\%$로 향상되었다.

## **Diagnosing image classification CNNs with Grad-CAM**

### **Analyzing failure modes for VGG-16**

network가 분류를 정확히 하지 못한 예시들에 대하여 Guided Grad-CAM을 사용하여 정답 class와 예측된 class를 시각화하였다. 몇몇 failure는 ImageNet classification에서 내재된 애매모호함 때문에 발생하였는데, 이는 network가 아예 잘못된 예측을 한다기보다는, 사진이 다른 class로 오분류될 수 있을 법한 애매모호함을 가지고 있다는 것을 의미한다.

### **Effect of adversarial noise on VGG-16**

<p style="text-align: center;">
  <img src="/images/grad_3.png" width="60%">
</p>

저자들은 ImageNet-pretrained VGG-16 model에 대해 adversarial image를 생성하여 모델이 이미지 내에서 나타나지 않은 category로 높은 확률 (>0.9999)을 assign 하고 이미지 내에 나타난 category로 낮은 확률을 assign 하도록 만들었다. 그 뒤 이미지에 나타난 category에 대해 Grad-CAM visualization을 만들었다. Figure5에서 나타난 것처럼, network는 이미지에 존재하는 category에 대해서 매우 낮은 확률로 예측하고 있으나, 그럼에도 불구하고 이것들의 위치는 정확하게 잡아내는 것을 확인할 수 있다. 이를 통해 Grad-CAM은 adversarial noise에 꽤 강건하다는 사실을 알 수 있다.

### **Identifying bias in dataset**

<p style="text-align: center;">
  <img src="/images/grad_4.png" width="60%">
</p>

저자들은 ImageNet-pretrained VGG-16 model을 의사와 간호사 binary classification task에 finetune 하였다. 결과적으로 trained model은 좋은 validation accuracy를 달성하였으나, 일반화에서는 성능이 떨어지는 것을 확인할 수 있었다. 모델 예측에 대한 Grad-CAM visualization은 모델이 간호사를 의사로부터 구별하는 데 있어서 사람의 얼굴과 머리 스타일을 보도록 학습되었음을 나타내며, 이는 gender stereotype을 학습하였다는 것을 의미한다.

Grad-CAM visualization으로부터 얻은 이러한 직관을 이용해서, 클래스 당 이미지의 수는 유지하면서 남성 간호사와 여성 의사의 이미지를 추가함으로써 training set에서의 bias를 감소시킬 수 있었다. 이는 Grad-CAM이 dataset에서의 bias를 확인하고 제거하는데 도움을 줄 수 있으며, 이는 더 나은 일반화와 공정하고 윤리적인 결과를 위해서 중요하다.

