---

title:  "[Paper Review] Spatial Transformer Networks"

excerpt: "STN 논문 리뷰"
categories:
  - CV
toc_label: "  목차"
toc: true
toc_sticky: true

date: 2023-03-21
last_modified_at: 2023-03-21
---

## Background

논문의 저자들은 기존의 CNN이 spatially invarient하지 못한 것을 CNN의 근본적인 한계로 주장한다. 이 때 spatially invarient하지 못하다는 것의 의미는 Rotation, Reflextion, Stretching, Contraction과 같은 데이터의 다양한 공간적 변화에 잘 대처하지 못하다는 의미이다.

그래서 저자들은 Spatial Transformer Network(STN)라는 새로운 모듈을 제안한다. STN은 CNN이 공간적인 변화를 무시하지 않고 데이터를 처리할 수 있도록 한다. STN은 이미지 내에서 관심영역(Region of Interest, ROI)을 찾고, 이를 재배치(re-sampling)하는 방식으로 동작한다. 이러한 STN 모듈은 이미지 분류, 검출, 세그멘테이션 등 다양한 컴퓨터 비전 태스크에서 적용할 수 있다.

## The **Concept of Spatial Transformer**

<p align="center">
  <img src="/images/stn_1.png" width="50%">
</p>

그림을 보면, 크기, 각도, 노이즈 등 각 특색을 지닌 입력(a)에 대해 spatial transformer는 (b)와 같이 사각형 영역을 찾아낸다. 그 뒤 (c)와 같이 변환시킨 뒤 fully connected network에 전달합니다. 결과적으로 classifier는 (d)와 같이 각 이미지에 대한 결과를 예측할 수 있게 된다. 

기존 CNN은 일반적으로 회전 변환 혹은 flip과 같은 augmentation을 진행함으로써 분류 성능을 높이고자 한다. 위와 같이 Spatial transformer을 동작시킨다면 굳이 이렇게 다양한 관점에서 augmentation을 진행할 필요 없이 알아서 중요한 부분에 잘 집중할 수 있도록 만들어 준다. 

Spatial transformer의 동작은 각 입력 데이터 샘플마다 달라지고, 특별한 supervision 없이도 학습 과정에서 습득된다. 즉, 사용된 모델의 end-to-end training 과정 중에 backpropagation을 통해 한꺼번에 학습된다는 점이 중요하다.


## **Structure of Spatial Transformer**

<p align="center">
  <img src="/images/stn_2.png" width="60%">
</p>

### Localisation Network

Localisation Network는 input feature map $U$에 적용할 transform의 parameter matrix $\theta$를 추정한다. 다른 표현으로 feature을 input으로 받아 transformation을 위한 parameter를 output으로 내는 네트워크라고 할 수 있다. 입력 $U∈R^{H×W×C}$은 가로 $W$, 세로 $H$, 채널 수 $C$를 갖는다.

공식은 아래와 같다. 

$\theta = f_{loc}(U)$

- $U$ : input feature map (H*W*C)
- $\theta$: transformation parameter

Localisation network은 regression에 해당하는 어떤 네트워크도 모두 가능하다. 예로  fully-connected network 또는 convolutional network 모두 가능하며, 마지막 단에 regression layer가 있어 transformation parameter $\theta$를 추정할 수 있기만 하면 된다.

이 논문의 실험에서 저자들은 layer 4개의 CNN을 쓰기도 했고 layer 2개의 fully connected network을 사용하기도 했다.

### Grid Generator

<p align="center">
  <img src="/images/stn_3.png" width="70%">
</p>

다음으로 위에서 얻은 $\theta$를 적용할 grid를 sampling할 필요가 있다. Grid generator는 추정한 $\theta$에 따라 input feature map에서 sampling 할 지점의 위치를 정해주는 sampling grid $T_{\theta}(G)$ 를 계산한다. 이 때 G는 grid를 의미한다.

출력 V의 각 pixel은 regular grid 형태의 G 위에 위치하고 있다. 이는 figure 3의 (a)와 같고 identity mapping이 된 모습이다. 출력 V의 sampling grid G는 transform $T_{\theta}$를 거쳐 입력 $U$의 sampling grid $T_{\theta}(G)$로 mapping되게 되며 이는 figure 3의 (b)와 같다. 

만일 $T_{\theta}$가 2D Affine Transformation($A_{\theta}$) 라면 아래의 식과 같이 표현 가능하다.

$$
\begin{pmatrix} x_i^s \\ y_i^s \end{pmatrix} = T_{\theta}(G_i) = A_{\theta} \begin{pmatrix} x_i^t \\ y_i^t \\ 1 \end{pmatrix} = 
\begin{bmatrix} 
\theta_{11} & \theta_{12} & \theta_{13} \\ 
\theta_{21} & \theta_{22} & \theta_{23} 
\end{bmatrix} 
\begin{pmatrix} x_i^t \\ y_i^t \\ 1 \end{pmatrix}
$$

Affine transform은 6개의 parameter로 이루어져 있으며, scale, rotation, translation, skew, cropping을 표현 가능하다. 이 때 위 식에서 $G_i$는 target grid로 $G_i = (x_i^t, y_i^t)$이다. 

추가로 attention model은 아래와 같이 표현 가능하다. 이는 3개의 parameter로 isotropic scale, translation, cropping의 표현이 가능하다.

$$
\begin{pmatrix} x_i^s \\ y_i^s \end{pmatrix} = T_{\theta}(G_i) = A_{\theta} \begin{pmatrix} x_i^t \\ y_i^t \\ 1 \end{pmatrix} = 
\begin{bmatrix} 
s & 0 & t_{x} \\ 
0 & s & t_{y} 
\end{bmatrix} 
\begin{pmatrix} x_i^t \\ y_i^t \\ 1 \end{pmatrix}
$$

$T_{\theta}$의 경우 각 parameter에 대해 미분이 가능하기만 하다면 위에서 살펴본 transform 이외의 transform도 모두 표현할 수 있다. 

### Sampler

Sampler는 input feature map $U$에 sampling grid $T_{\theta}(G)$를 적용하여 변환된 output feature map $V$를 만든다. 변환된 $V$에서 특정 pixel 값을 얻기 위해 입력 $U$의 어느 위치에서 값을 가져올지를 선택하는 과정이며 식으로는 일반적으로 아래와 같이 표현된다.

$$
V_i^c=\sum_n^H\sum_m^WU^c_{nm}k(x^s_i-m;\Phi_x)k(y^s_i-m;\Phi_y) ,\ \forall i \in [1...H'W'] \ \forall c \in [1...C]
$$

선택한 위치가 정확히 정수 좌표 값을 갖지 않을 가능성이 높기 때문에 interpolation을 적용하고, interpolation을 구현하는 함수는 식에서 sampling kernel $k$로 표시되었다. $U$와 $G$에 대해 미분 가능하다면 어떤 sampling kernel로도 표현 가능하다. 

만약 bilinear interpolation을 적용하여 bilinear sampling kernel을 사용하였다면 아래와 같은 식이 된다.

$$
V_i^c=\sum_n^H\sum_m^WU^c_{nm}max(0,1-\vert x^s_i-m \vert)max(0,1-\vert y^s_i-n \vert)
$$

전체 네트워크에서 backpropagation을 계산하기 위해서는 $U$와 $G$에 대해 미분 가능해야 한다. Bilinear interpolation의 경우 각각의 partial derivative를 구해보면 아래 식과 같다. 

$$
{{\partial V_i^c} \over {\partial U^c_{nm}}}=\sum_n^H\sum_m^Wmax(0,1-\vert x^s_i-m \vert)max(0,1-\vert y^s_i-n \vert)
$$

$$
\frac{\partial V_i^c}{\partial x_i^s} = \sum_{n=1}^H \sum_{m=1}^W U_{nm}^c \max(0, 1 - |y_i^s - n|) 
\begin{cases}
0 & \text{if } |m - x_i^s| \geq 1 \\
1 & \text{if } m \geq x_i^s \\
-1 & \text{if } m < x_i^s
\end{cases}
$$

마찬가지로 ${{\partial V_i^c} \over {\partial y^s_i}}$의 식도 구할 수 있다. Sampling function이 모든 구간에서 미분가능하지 않아도 구간별로 나눠 backpropagation을  sub-gradient를 통하여 계산 가능하다. 

### Spatial transformer networks

위에서 살펴본 localisation → Grid Generator → Sampling으로 구성된 Spatial transformer는 네트워크 구조 어디든 삽입되어 사용 가능하며, 이를 CNN 네트워크에 추가하였을 때 이를 Spatial Transformer Network라고 한다. Spatial Transformer는 이론적으로 네트워크 구조의 어느 지점에 몇 개라도 추가 가능하다. 

Spatial Transformer를 추가하더라도 parameter나 계산량이 크게 증가하지 않는 장점을 갖고 있으며, 또한 이를 학습하기 위한 별도의 loss는 필요하지 않고 기존 task loss를 사용하여 학습되기 때문에 전체 training 속도에 미치는 영향이 거의 없다는 것도 장점이다. 

Spatial transformer module을 CNN의 입력 바로 앞에 배치하는 것이 가장 일반적이지만, network 내부의 깊은 layer에 배치해 좀더 추상화된 정보에 적용을 한다거나 여러 개를 병렬로 배치해서 한 image에 속한 여러 부분을 각각 tracking하는 용도로 사용할 수도 있다.

## Experiments

### Distorted MNIST

<p align="center">
  <img src="/images/stn_4.png" width="80%">
</p>

Distorted된 MNIST 데이터셋에 대한 실험으로 데이터는 rotation (**R**), rotation-translation-scale (**RTS**), projective transformation (**P**), elastic warping (**E**)의 방식으로 distorted 되었다. 실험 결과 ST-CNN의 성능이 가장 좋은 것을 알 수 있으며 같은 모델 내에서는 TPS transformation이 가장 좋은 성능을 보였다. figure 4를 보면 뒤틀려 있는 이미지들이 잘 align 된 것을 볼 수 있다.

### Street View House Numbers

<p align="center">
  <img src="/images/stn_5.png" width="80%">
</p>

20만 개의 실제 집 주소 표지의 사진으로 구성된 데이터셋에서 숫자를 인식하는 실험으로 Figure 5의 (a)에서 볼 수 있듯 CNN의 convolutional stack 부분에 복수의 spatial transformer를 삽입해서 사용했다. 결과적으로 Convolution layer마다의 STN 삽입을 통해 성능이 향상되는 것을 볼 수 있었으며 가장 높은 성능을 보인 ST-CNN multi 모델의 경우 기본 CNN 모델보다 6%만 느려짐을 확인하였다.

### Fine-Grained Classification

<p align="center">
  <img src="/images/stn_6.png" width="80%">
</p>

마지막은 200종의 새 사진 11,788장으로 구성된 Caltech의 CUB-200-2011 birds 데이터셋에 fine-grained bird classification을 적용한 실험이다. 결과적으로 모든 ST-CNN이 기존 CNN보다 높은 성능을 보였으며 기존의 image class label만으로 학습했음에도 성능이 향상됨을 확인할 수 있었다.


