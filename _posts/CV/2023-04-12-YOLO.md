---

title:  "[Paper Review] You Only Look Once: Unified, Real-Time Object Detection"

excerpt: "YOLO 논문 리뷰"
categories:
  - CV
toc_label: "  목차"
toc: true
toc_sticky: true

date: 2023-04-12
last_modified_at: 2023-04-12
---

## Background

기존의 검출(detection) 모델은 분류기(classfier)를 재정의하여 검출기(detector)로 사용하고 있다. 기존의 객체 검출 모델로는 대표적으로 DPM과 R-CNN이 있다.

Deformable parts models(DPM)은 이미지 전체를 거쳐 sliding window 방식으로 객체 검출을 하는 모델이다. R-CNN은 region proposal 방식을 사용하여 bounding box를 생성한다. 제안된 bounding box에 classifier를 적용하여 분류(classification)한 뒤 bounding box를 조정하고, 중복된 검출을 제거하고, 객체에 따라 box의 점수를 재산정하기 위해 후처리(post-processing)를 진행한다. 이런 복잡함 때문에 R-CNN은 느리고, 각 절차를 독립적으로 훈련시켜야 하므로 최적화(optimization)하기도 어렵다.

따라서 YOLO 연구진은 이미지의 픽셀로부터 bounding box의 위치(coordinates), 클래스 확률(class probabilities)을 구하기까지의 일련을 절차를 하나의 회귀 문제로 재정의하였다. 이러한 시스템을 통해 YOLO(you only look once)는 이미지 내에 어떤 물체가 있고 그 물체가 어디에 있는지를 하나의 파이프라인으로 빠르게 구해준다.

YOLO의 경우 단일 신경망 구조이기 때문에 구성이 단순하며, 빠르다. 또한 주변 정보까지 학습하며 이미지 전체를 처리하기 때문에 background error가 작으며, 훈련 단계에서 보지 못한 새로운 이미지에 대해서도 검출 정확도가 높다. 단, 속도와 정확성은 trade-off 관계로, YOLO는 SOTA 객체 검출 모델에 비해 정확도(mAP)가 다소 떨어진다.

## Unified Detection

YOLO는 입력 이미지(input images)를 $S x S$ grid로 나눈다. 각각의 그리드 셀(grid cell)은 B개의 bounding box와 그 bounding box에 대한 confidence score를 예측한다. confidence score는 bounding box가 객체를 포함한다는 것을 얼마나 믿을만한지, 그리고 예측한 bounding box가 얼마나 정확한지를 나타낸다.
confidence score는 다음과 같이 정의된다.

$$
Pr(Object) *IOU_{pred}^{truth}
$$

여기서 IOU는 intersection over union의 약자로 객체의 실제 bounding box와 예측 bounding box의 합집합 면적 대비 교집합 면적의 비율을 뜻한다.

각각의 bounding box는$ x, y, w, h, confidence$의 5개의 예측치로 구성되어 있다. $(x, y)$ 좌표 쌍은 bouning box 중심의 그리드 셀(grid cell) 내 상대 위치를 의미하고, $(w, h)$ 쌍은 bounding box의 상대 너비와 상대 높이를 의미한다.

각각의 그리드 셀은 conditional class probabilities(C)를 예측한다. conditional class probabilities는 그리드 셀 안에 객체가 있다는 조건 하에 그 객체가 어떤 클래스(class)인지에 대한 조건부 확률로, 다음과 같이 계산할 수 있다.

$$
C(conditional \\ class \\ probablilities) = Pr(Class_i|Object)
$$

테스트 단계에서는 conditional class probability(C)와 개별 boudning box의 confidence score를 곱해주는데, 이를 각 bounding box에 대한 class-specific confidence score라고 한다. class-specific confidence score는 위에서 구한 conditional class probability와 confidence score를 곱하여 계산할 수 있다.

$$
class-specific \\ confidence \\ score 
= Pr(Class_i|Object)*Pr(Object)*IOU_{pred}^{truth}
= Pr(Class_i)*IOU_{pred}^{truth} 
$$

class-specific confidence score는 bounding box에 특정 클래스(class) 객체가 나타날 확률과 예측된 bounding box가 그 클래스(class) 객체에 얼마나 잘 들어맞는지를 나타낸다.

<p style="text-align: center;">
  <img src="/images/yolo_1.png" width="60%">
</p>

YOLO 연구진은 pascal  VOC 데이터셋을 사용하여 실험했다. S=7, B=2로 세팅했고 파스칼 VOC는 총 20개의 라벨링 된 클래스가 있으므로 C=20이다. S=7 이면 인풋 이미지는 7 x 7 그리드로 나뉜다는 뜻이고, B=2이라는 것은 하나의 그리드 셀에서 2개의 bounding box를 예측하겠다는 뜻이다. 이렇게 했을 때 S x S x (B*5 + C) 텐서를 생성한다. 따라서 최종 예측 텐서의 dimension은 (7 x 7 x 30)가 된다.

### Network Design

YOLO 모델은 하나의 CNN 구조로 디자인되었다. CNN의 앞단은 컨볼루션 계층(convolutional layer)이고, 이어서 전결합 계층(fully-connected layer)으로 구성되어 있다. 컨볼루션 계층(convolutional layer)은 이미지로부터 특징을 추출하고, 전결합 계층(fully connected layer)은 클래스 확률과 bounding box의 좌표(coordinates)를 예측한다.

<p style="text-align: center;">
  <img src="/images/yolo_2.png" width="80%">
</p>

YOLO의 신경망구조는 GoogLeNet에서 따왔으며, 총 24개의 컨볼루션 계층(convolutional layers)과 2개의 전결합 계층(fully connected layers)으로 구성되어 있다. GoogLeNet의 인셉션 구조 대신 YOLO는 1 x 1 축소 계층(reduction layer)과 3 x 3 컨볼루션 계층의 결합을 사용했다.YOLO 모델의 전체 구조는 위와 같으며, 이 모델의 최종 아웃풋은 7 x 7 x 30의 예측 텐서(prediction tensors)이다. 

### Training

우선, 1,000개의 클래스를 갖는 ImageNet 데이터 셋으로 YOLO의 컨볼루션 계층을 사전훈련(pretrain)시켰다. 사전훈련을 위해서 24개의 컨볼루션 계층 중 첫 20개의 컨볼루션 계층만 사용했고, 이어서 전결합 계층을 연결했다.

ImageNet은 분류(classification)를 위한 데이터 셋이다. 따라서 사전 훈련된 분류 모델을 객체 검출(object detection) 모델로 바꾸어야 하는데, 연구진은 사전 훈련된 20개의 컨볼루션 계층 뒤에 4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가하여 성능을 향상시켰다. 4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가할 때, 이 계층의 가중치(weights)는 임의로 초기화했다. 또한, 객체 검출을 위해서는 이미지 정보의 해상도가 높아야 하므로 입력 이미지의 해상도를 224 x 224에서 448 x 448로 증가시켰다.

이 신경망의 최종 아웃풋(예측값)은 클래스 확률(class probabilities)과 bounding box 위치정보(coordinates)이다. bounding box의 위치정보에는 bounding box의 너비(width)와 높이(height)와 bounding box의 중심 좌표(x, y)가 있다. YOLO 연구진은 너비, 높이, 중심 좌표값(w, h, x, y)을 모두 0~1 사이의 값으로 정규화(normalize)했다.

신경망의 마지막 계층에는 선형 활성화 함수(linear activation function)를 적용했고, 나머지 모든 계층에는 leaky ReLU를 적용했다.

YOLO의 경우 localization loss, classification loss의 2가지 loss가 존재하고, 이미지 내 대부분의 그리드 셀에는 객체가 없다는 구조적 문제점이 있다. 이를 개선하기 위하여 아래 3가지 개선안을 적용하였다.

(1) localization loss와 classification loss 중 localization loss의 가중치를 증가시킨다.

(2) 객체가 없는 그리드 셀의 confidence loss보다 객체가 존재하는 그리드 셀의 confidence loss의 가중치를 증가시킨다. 이를 위해 두 개의 파라미터를 사용했는데, $λ_{coord}$와 $λ_{noobj}$이다.  $λ_{coord}=5$, $λ_{noobj}$=0.5로 가중치를 주었다.

(3) bounding box의 너비(widht)와 높이(hegith)에 square root를 취해준 값을 loss function으로 사용한다. 너비와 높이에 square root를 취해주면 너비와 높이가 커짐에 따라 그 증가율이 감소해 loss에 대한 가중치를 감소시키는 효과가 있다.

추가로 과적합(overfitting)을 막기 위해 드롭아웃(dropout)과 data augmentation을 적용하여 Training을 진행하였다.

### Inference

훈련 단계와 마찬가지로, 추론 단계에서도 테스트 이미지로부터 객체를 검출하는 데에는 하나의 신경망 계산만 하면 된다. 파스칼 VOC 데이터 셋에 대해서 YOLO는 한 이미지 당 98개의 bounding box를 예측하고, 그 bounding box마다 클래스 확률(class probabilities)을 구한다.

하지만 YOLO의 그리드 디자인(grid design)은 한 가지 단점이 있는데, 하나의 객체를 여러 그리드 셀이 동시에 검출하는 경우가 있다는 점이다. 이를 다중 검출(multiple detections) 문제라고 한다. 이런 다중 검출(multiple detections) 문제는 비 최대 억제(non-maximal suppression)라는 방법을 통해 개선할 수 있다. YOLO는 비 최대 억제를 통해 mAP를 2~3%가량 향상시켰다.

### Limitations

YOLO는 하나의 그리드 셀마다 두 개의 bounding box를 예측한다. 그리고 하나의 그리드 셀마다 오직 하나의 객체만 검출할 수 있다. 이는 공간적 제약(spatial constraints)을 야기한다. 공간적 제약이란 '하나의 그리드 셀은 오직 하나의 객체만 검출하므로 하나의 그리드 셀에 두 개 이상의 객체가 붙어있다면 이를 잘 검출하지 못하는 문제'를 뜻한다.

그리고 YOLO 모델은 데이터로부터 bounding box를 예측하는 것을 학습하기 때문에 훈련 단계에서 학습하지 못했던 새로운 종횡비(aspect ratio, 가로 세로 비율)를 마주하면 잘 예측하지 못한다.

마지막으로 YOLO 모델은 큰 bounding box와 작은 bounding box의 loss에 대해 동일한 가중치를 둔다. 큰 bounding box에 비해 작은 bounding box가 위치 변화에 따른 IOU 변화가 더 심하기 때문에 성능에 큰 영향을 줄 수 있으며, 이를 부정확한 localization 문제라고 한다.

## Experiments

### Comparison to Other Real-Time Systems

<p style="text-align: center;">
  <img src="/images/yolo_3.png" width="50%">
</p>

위 표는 각종 객체 검출 모델 별 정확도(mAP)와 속도(FPS)를 보여준다. 정확도는 Fast R-CNN과 Faster R-CNN VGG-16이 가장 높지만 이 모델들의 FPS는 너무 낮아 실시간 객체 검출 모델로 사용하기에는 어렵다. 반면, 정확도도 적당히 높고 속도도 빠른 모델은 YOLO 계열인 것을 알 수 있다.

### VOC 2007 Error Analysis

파스칼 VOC 2007 데이터 셋에 대해 YOLO와 Fast R-CNN의 성능을 비교해보면 아래와 같다.

<p style="text-align: center;">
  <img src="/images/yolo_4.png" width="50%">
</p>

YOLO는 localization error가 19.0%로 나머지 error를 모두 합한 15.5%(6.75%+4.0%+4.75%) 보다 크다. Fast R-CNN은 YOLO에 비해 localization error가 작은 반면, background error가 YOLO에 비해 3배 더 크다.

### Combining Fast R-CNN and YOLO

YOLO는 Fast R-CNN에 비해 background error가 훨씬 적다. 따라서 Fast R-CNN에 YOLO를 결합하여 background error를 줄인다면 굉장히 높은 성능을 낼 수 있을 것이다. 파스칼 VOC 2007 데이터 셋에 대해 가장 성능이 좋은 Fast R-CNN 모델은 71.8%의 mAP를 기록했으나, Fast R-CNN과 YOLO를 결합하면 mAP가 3.2% 올라 75.0%가 된다. 물론 Fast R-CNN과 YOLO를 결합한 모델은 YOLO에 비해 느리지만 Fast R-CNN을 단독으로 돌리는 것과 앙상블 모델을 돌리는 것의 속도는 거의 유사하므로, Fast R-CNN을 단독으로 사용하는 것보다는 Fast R-CNN과 YOLO를 결합한 모델을 사용하는 것이 더 낫다고 볼 수 있다.

### VOC 2012 Results

<p style="text-align: center;">
  <img src="/images/yolo_5.png" width="80%">
</p>

파스칼 VOC 2012 데이터 셋에서 YOLO는 57.9%의 mAP를 달성했다. 속도 측면에서는 YOLO가 빠르고, 정확도 측면에서는 Fast R-CNN과 YOLO를 결합한 모델이 가장 좋다.

### **Generalizability: Person Detection in Artwork**

<p style="text-align: center;">
  <img src="/images/yolo_6.png" width="80%">
</p>

<p style="text-align: center;">
  <img src="/images/yolo_7.png" width="80%">
</p>

YOLO 연구진은 훈련 데이터 셋과 다른 분포는 지닌 테스트 데이터 셋(즉, 훈련 데이터 셋에서 보지 못한 새로운 데이터 셋)을 활용하여 테스트를 진행하였다. 테스트셋으로 피카소 데이터 셋과 일반 예술 작품을 사용했다. 결과적으로 다른 모델에 비하여 YOLO는 훈련 단계에서 접하지 못한 새로운 이미지도 잘 검출한다는 것을 알 수 있다.




