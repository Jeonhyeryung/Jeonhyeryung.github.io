---

title:  "[Paper Review] GeoNeRF : Generalizing NeRF with Geometry Priors"

excerpt: "GeoNeRF 논문 리뷰"
categories:
  - CV
toc_label: "  목차"
toc: true
toc_sticky: true

date: 2023-08-29
last_modified_at: 2023-09-01
---

> **GeoNeRF is a generalized NeRF model that renders and synthesizes new perspectives using neural radiance fields. It generates more consistent geometric details and provides more realistic images compared to existing NeRF models.**
🎥 GeoNeRF is a generalized NeRF model that renders new perspectives using neural radiance fields.
🌍 GeoNeRF generates geometric details consistently to provide more realistic images.
> 

## Summary

---

1. **Feature Pyramid Networks (FPN)**: 2D 이미지 feature를 추출, 여러 단계의 특징을 분석하여 상세한 정보를 취합.
2. **Homography Warping**: 추출된 2D 특징을 기반으로  homography warping을 사용하여 3D cost volume을 생성.
3. **3D CNN**: 이 3D cost volume을 기반으로 3D Convolutional Neural Networks($R_{3D}^{(l)}$)을 사용하여 깊이 맵(depth map)과 3D feature map를 추정.
4. **Ray Sampling and Interpolation**: 렌더링할 새로운 뷰에서 광선을 샘플링하고, 이를 기반으로 2D와 3D feature를 보간(interpolate).
5. **Multi-Head Self-Attention**: 뷰에 독립적인 토큰과 뷰에 종속적인 토큰을 생성하고, 이를 MHA를 활용하여 토큰 출력.
6. **Volume Rendering**: 최종적으로, 볼륨 렌더링 기술을 사용하여 광선의 색상을 계산.

## Main Idea

---

- Generalizable model을 만들어서 적은 수의 학습 데이터로도 학습을 진행해보자
- Geometry Reasoner, MHA, AE 등을 활용

## Method

---

### 1. Geometry Reasoner

**Input:** 

- **근접한 뷰(Views)의 집합**: 크기가 $H \times W$인 주변 뷰 $\{I_v\}_{v=1}^V$
- **Semantic 2D features**: Feature Pyramid Network (FPN)를 통해 생성된 다양한 스케일 레벨에 대한 Semantic 2D features $f_v^{(l)}$

$$
f_v^{(l)} = \mathrm {FPN} (I_v) \in \mathbb R^{{H \over 2^l} \times {W \over 2^l} \times 2^l C } \ \ \ \ \  \forall l \in \{0,1,2 \}
$$

**Output:**

- **깊이 맵(Depth Maps)**: 각 입력 이미지에 대한 다양한 스케일 레벨에서의 깊이 맵 $\hat D^{(l)}$
- **3D 특징 맵**(**3D Feature Maps)**: 다양한 스케일 레벨에서의 Semantic 3D features $\Phi^{(l)}$

$$
\hat D_v^{(l)}, \Phi_v^{(l)} = R_{3D}^{(l)}(P_v^{(l)}) \ \ \ \ \  \forall l \in \{0,1,2 \}
$$

**역할:**

1. **특징 추출**: 각 입력 이미지를 Feature Pyramid Network (FPN)을 통해 처리하여 다양한 스케일에서의 Semantic 2D features를 추출한다.
2. **Cost Volume 생성**: 추출된 2D features를 기반으로 다양한 깊이 레벨에서의 cost volume을 생성한다. 이 때 "group-wise correlation similarity" 메트릭을 사용하여 메모리 사용량과 추론 시간을 줄인다.
3. **깊이 및 특징 맵 생성**: 생성된 cost volume을 3D hourglass 네트워크를 통해 further process 및 정규화하고, 깊이 맵과 3D 특징 맵을 최종적으로 생성한다.

**주요 구성요소:**

- **Feature Pyramid Network (FPN)**: 2D 시맨틱 특징을 추출하는 데 사용.
- **CasMVSNet 아키텍처**: 깊이 맵을 생성하는 기본 구조로 사용되며, 여기에 몇 가지 수정을 가해 geometry reasoner로 활용.
- **3D Hourglass Networks**: 생성된 cost volumes를 further process 및 정규화하는 데 사용.

### 2. Sampling Points on a Novel Ray

→ 새로운 시점에서 광선을 렌더링 할 때 사용되는 방법

**Input:** 

- Geometry Reasoner로부터 생성된 feature들
- 새로운 카메라 포즈에서의 광선(ray)

**Output:**

- 광선을 렌더링하기 전에 샘플링할 값들의 조합 $N = N_c + N_f$

**역할:**

1. 새로운 카메라 포즈에서의 광선에 대해 깊이 범위를 전체적으로 다룰 수 있도록 포인트를 
샘플링
2. full-resolution partial cost volume $\mathbf P^{(0)}$ 에서 가장 중요한 정보를 가진 포인트를 추가적으로 
샘플링

**주요 구성요소:**

- **Ray Casting**: 새로운 뷰를 렌더링할 때 사용되는 기술
- $N_c$: 깊이 범위를 전체적으로 다룰 수 있도록 광선을 따라 일정하게 샘플링된 포인트의 수
- $N_f$: full-resolution partial cost volume $\mathbf P^{(0)}$ 에서 추가적으로 샘플링하는 포인트의 수로, 이 부분은 표면과 기하학에 대한 가장 중요한 정보를 포함하고 있음
- **Stepwise Probability** $p_0(x)$: 포인트 $x$가 고해상도 부분 비용 볼륨 내부에 포함될 확률을 추

### 3. Renderer

**Input:** 

- **샘플링된 점들의 집합:** 각 광선을 따라 균일하게 샘플링 된 $N_c$와 Geometry reasoner의 출력을 기반으로 샘플링 된 $N_f$, 총 $N = N_c + N_f$개의 포인트들의 집합 $\{ x_ n \}_{n=1}^N$
- **Semantic 2D features: $f_{n,v}^{(0)}$**
- **3D 특징 맵**(**3D Feature Maps)**: $\{ \Phi_{n,v}^{(l)} \}_{l=1}^2$
- **Occlusion Mask: $M_{n,v}$**

**Output:**

- **볼륨 밀도의 예측 $\sigma_n$**
- **색상의 예측 $\hat c_n$**
- **광선에 대한 추정 깊이** $\hat d$

**실행 단계:**

1. **특징 보간**: 먼저, renderer는 모든 원본 뷰에서 얻은 다양한 해상도와 레벨의 특징들을 각 샘플 점에서 보간
2. **뷰 독립적 & 뷰 의존적 토큰 생성**: 보간된 특징을 사용하여 각 샘플 점에 대한 뷰 독립적인 토큰과 뷰 의존적인 토큰을 생성, 이 토큰들은 장면의 전역적인 이해와 원본 뷰에서의 이해를 나타낸다.
    - 뷰 의존적인 토큰 $t_{n,v}$ 생성
        
        $$t_{n,v} = \mathrm {LT}([f_{n,v}^{(0)};\{ \Phi_{n,v}^{(l)} \}]_{l=0}^2) \ \ \ \ \ \forall v \in \{  1,...,V \}$$
        
    - 뷰 독립적인 코튼 $t_{n,0}$ 생성
        
        $$t_{n,0} = \mathrm {LT} ([mean\{f_{n,v}^{(0)} \}_{v=1}^V ; var\{f_{n,v}^{(0)}\}_{v=1}^V])$$
        
3. **Multi-Head Attention을 통한 토큰 집계**: 뷰 독립적인 토큰과 뷰 의존적인 토큰은 Multi-Head Attention(MHA) 레이어를 통해 집계되고, 이 과정에서 occlusion 마스크도 고려되어, 가려진 뷰가 결과에 기여하지 않도록 한다.
    
    $$\{t'_{n,v} \}_{v=0}^ V = ^{4 \times}\mathrm{MHA} (t_{n,0}, \{ t_{n,v}, M_{n,v} \}_{v=1}^V)$$
    
4. **볼륨 밀도 예측**: 집계된 토큰을 사용하여 장면의 볼륨 밀도를 예측. 이는 오토인코더 스타일의 네트워크를 통해 수행된다.
    
    $$\{\sigma_n \}_{n=1}^N = \mathrm{MLP}_{\sigma} (\mathrm{AE}(\{ t'_{n,0} \}_{n=1}^N))$$
    
5. **색상 예측**: 뷰 의존적인 토큰과 추가 입력을 사용하여 각 샘플 점의 색상을 예측한다.
    - 각 뷰 $v$에 대해, 뷰 의존적인 토큰 $t_{n,v}$과 $\theta_{n,v}$를 입력으로 사용하는 MLP를 통과시켜 가중치 $w_{n,v}$를 계산, Softmax 함수도 가중치 계산에 사용되며, occlusion mask $M_{n,v}$를 입력으로 받아 occlusion view는 제외됨
    
    $$w_{n,v} = \mathrm{Softmax}(\{\mathrm{MLP}_c([t'_{n,v} ; \gamma(\theta_{n,v})]-), M_{n,v} \}_{v=1}^V)$$
    
    - 각 샘플 점 $x_n$의 예측된 색상 $\hat c_n$을 계산,  $c_{n,v}$는 원본 뷰의 이미지 평면에서 특정 포인트 $x_n$을 project한 후에 색상 샘플을 보간하여 얻어짐
    
    $$\hat c_n = \sum_{v=1}^V w_{n,v}c_{n,v} \ \ \ \ \ \forall n \in \{ 1,2,..., N\}$$
    
6. **볼륨 렌더링을 통한 최종 이미지 생성**: 예측된 볼륨 밀도와 색상을 사용하여 실제 3D 장면을 2D 이미지로 렌더링하고, 이 과정에서 카메라 광선의 색상과 깊이도 추정된다. 
    
    $$\hat c = \sum_{n=1}^N \exp(-\sum_{k=1}^{n-1} \sigma_k)(1-\exp(-\sigma_n)) \hat c_n$$
    
    - $\hat z_n$은 샘플 포인트  $x_n$의 신규포즈(novel pose)에 대한 깊이
    
    $$\hat d = \sum_{n=1}^N \exp(-\sum_{k=1}^{n-1} \sigma_k)(1-\exp(-\sigma_n)) \hat z_n$$
    
    

**역할:**

- 레이의 각 점에 대해 색상과 볼륨 밀도를 예측
- 뷰에 독립적인 특징과 뷰에 종속적인 특징을 활용해 예측을 수행
- 레이의 전체 색상과 깊이를 렌더링

**주요 구성요소:**

1. **Linear Transformation (LT)**: 특징을 선형 변환
2. **Multi-Head Attention (MHA) Layers**: 뷰에 독립적인 토큰과 뷰에 종속적인 토큰을 집계
3. **Occlusion Masks $M_{n,v}$**: 가려지는 뷰를 처리
4. **Auto-Encoder-Style (AE) Network**: 전역 기하학을 학습하고 볼륨 밀도를 예측
5. **Softmax Function**: 가중치를 계산하여 색상을 예측
6. **Volume Rendering Approach**: 볼륨 밀도와 색상을 기반으로 광선의 색상을 렌더링

### 4. Loss Functions

**Input:** 

광선(ray), 카메라 포즈, 원본 이미지에서의 색상 샘플, 깊이 정보 등

**Output:**

렌더링된 색상, 예측된 깊이, 뷰 종속 토큰 등

**역할:**

- **기본 손실 함수**: 렌더링된 색상과 실제 픽셀 색상 사이의 평균 제곱 오차
    
    $$\mathcal L_c = {1 \over |R|} \sum_{r \in R} \Vert \hat c (r) - c_{gt}(r) \Vert ^2$$
    
    
- **깊이 손실 함수:** 예측된 깊이와 실제 깊이 정보가 있을 경우 이를 이용해 깊이 예측을 정확하게 함
    
    $$\mathcal L_d = {1 \over |R_d|} \sum_{r \in R_d} \Vert \hat d (r) - d_{gt}(r) \Vert_{s1}$$
    
    
- **Geometry Reasoner 손실 함수:** 기하 정보를 더 정확하게 추론할 수 있도록 함
    
   
    $$\mathcal L_D^{(l)} = {2^{-l} \over |V|} \sum_{v=1}^V  \left\langle \Vert \hat D_v^{(l)}-D_v^{(l)} \Vert_{s1} \right\rangle$$
    
    
- **최종 손실 함수:** 최종적으로 위의 모든 손실 함수를 결합, $\lambda$는 실제 깊이 정보가 있을 경우 1.0, 그렇지 않은 경우 0.1
    - fine tuning 시 $\mathcal L_c$만을 손실 함수로 사용
    
    $$\mathcal L = \mathcal L_c + 0.1 \mathcal L_d + \lambda \sum_{l=0}^2 \mathcal L_D ^{(l)}$$
    

**주요 구성요소:**

- **레이(R)**: 훈련 배치에서의 레이 집합
- **실제 색상 $c_{gt}$ 및 실제 깊이 $d_{gt}$**: 실제 픽셀의 색상과 깊이 값
- **예측 생상 $\hat c$ 및 예측 깊이 $\hat d$** : 모델이 예측한 색상과 깊이 값
- **깊이 맵** $D_v^{(l)}$: 각 뷰에서의 실제 깊이 맵
- **Smooth $L_1$ 손실**: 깊이 손실을 계산할 때 사용되는 손실 함수
- **가중치** $\lambda$: 실제 깊이 정보가 있을 경우와 그렇지 않을 경우를 구분하는 가중치

### 5. Compatibility with RGBD data

→ GeoNeRF 모델을 RGBD(색상+깊이) 데이터와 호환되도록 수정한 GeoNeRF $_{+D}$ 모델을 제안

**Input:** 

각 소스 뷰(source view)에 대한 불완전하고 저해상도, 잡음이 있는 깊이 맵 $D_v$

**Output:**

깊이 정보가 추가로 고려된 3D 복원 결과

**역할:**

- $B_v$: $D_v$를 이용하여 생성된 이진 볼륨, GeoNeRF $_{+D}$에서 기하학의 대한 가이드 역할

**주요 구성요소:**

- **깊이 맵** $D_v$: 각 원본 뷰에 대한 저해상도 깊이 맵
- **이진 볼륨** $B_v$: $D_v$를 바탕으로 생성되며, 특정 깊이 평면이 실제 깊이와 일치하면 1, 그렇지 않으면 0을 가짐
- $Q(\cdot)$: 실제 깊이 값을 깊이 평면 지수로 매핑하고 양자화하는 함수

## Result

---

<p align="center">
  <img src="/images/geonerf_1.png">
</p>

- 비교군들이 Generalizable Model들로 되어 있어서, 전반적으로 PSNR이 낮은 수치
- fine-tuning을 진행한 per-scene optimization과 fine-tuning을 진행하지 않은 No per-scene optimization으로 구성
- Generalizable Model에서도 GeoNeRF의 성능이 높음을 볼 수 있음

<p align="center">
  <img src="/images/geonerf_2.png">
</p>

- source view의 갯수에 따른 성능차이 → source view가 많을 수록 성능이 높아짐
