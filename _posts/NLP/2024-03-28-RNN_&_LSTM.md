---

title:  "[Paper] Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network"

excerpt: "RNN, LSTM 논문 리뷰"
categories:
  - NLP
toc_label: "  목차"
toc: true
toc_sticky: true

date: 2023-05-10
last_modified_at: 2023-05-10
---

## Sequence and Sequence model

### Sequence

순서가 있는 data를 Sequence라고 한다. 예를 들어  Text data, 시계열 데이터 등이 있다. 이외에도 영상, 음성 등도 전부 순서와 함께 흘러가는 데이터로 Sequence data에 속한다.

### Sequence model

Sequence data를 다루는 model을 Sequence model이라고 한다. 기존에는 모든 input data들이 독립적이라고 가정했다. 그러나 이전 정보가 있는 Sequence data를 다룰 때는 순차적으로 과거 정보를 반영할 수 있는 모델이 필요하다. 그래서 나온 것이 Sequence model로, 순서가 있는 Sequence data에서 특징들을 추출하여 여러가지 문제를 해결하고 예측한다.

Seqeunce model이 해결하는 task는 4가지로 나눠볼 수 있다. 

<p style="text-align: center;">
  <img src="/images/rnn_1.png" width="70%">
</p>

첫 번째는 one to many이다. 이는 이미지 데이터에서 설명글을 출력하는 Image Captioning 등을 예시로 들 수 있다. 다음은 many to one이다. 텍스트에서 감정이 긍정적인지 부정적인지 분류하는 Sentiment Classification이나 어떤 지역의 최근 날씨가 주어졌을 때 향후 날씨를 예측하는 것과 같은 시계열 예측 등이 이 task에 속한다. 다음은 many to many로 encoder와 decoder을 거치는 과정이 존재하며, 영어 문장을 한글 문장으로 변환하는 Machine Translation을 예시로 들 수 있다. 마지막 many to many는 Text에서 언급된 사람, 장소 등의 개체를 인식하여 출력하는 Name entity recognition 또는 Video classification on frame level등을 예시로 들 수 있다. 

이러한 task를 수행하는 Sequence model에는 대표적으로 RNN, LSTM 등이 있다.

## Recurrent Neural Networks(RNN)

### Basic structure of RNN

RNN은 히든 노드가 방향을 가진 엣지로 연결되어 순환 구조를 이루는 인공 신경망의 한 종류이다. 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점이다.

<p style="text-align: center;">
  <img src="/images/rnn_2.png" width="50%">
</p>

RNN의 기본 구조는 Figure2와 같다. 녹색박스는 hidden state($h$)를 의미한다. 빨간 박스는 인풋 $x$, 파란 박스는 아웃풋 $y$이다. 현재 상태의 hidden state $h_t$는 직전 시점의 hidden state $h_{t-1}$을 받아 update된다. 

### Forward pass of RNN

<p style="text-align: center;">
  <img src="/images/rnn_4.png" width="70%">
</p>

RNN의 forward compute pass는 Figure3과 같이 진행된다. 먼저 sequence data는 vector의 형태로 embedding되어 model에 input된다. 다음으로 input vector는 timestep에 따라 순환 신경망의 state($h$)에 저장된다. t 시점에서의 state는 $h_t = tanh(W_{hh}h_{t-1}+W{ht}x_t+b_h)$와 같은 함수의 형태로 나타낼 수 있다. 이렇게 매 timestep마다 새로운 input이 들어오면서 다른 결과를 출력하게 되고, 이와 같은 연산을 통하여 최종적으로 output $y_t$가 계산된다. $y_t$는 $y_t = W_{hy}h_t+b_y$와 같은 함수의 형태로 나타낼 수 있다.

### Backward pass of RNN

<p style="text-align: center;">
  <img src="/images/rnn_5.png" width="70%">
</p>

RNN의 backward compute pass는 Figure4와 같이  진행된다. forward pass를 따라 최종적으로 $y_t$가 계산되면 최종 Loss에 대한 $y_t$의 그래디언트 $dy_t$가 덧셈 그래프를 타고 양방향에 모두 그대로 분배가 된다. $dW_{hy}$는 흘러들어온 그래디언트 $dy_t$에 로컬 그래디언트 $h_t$를 곱해 구한다. $dh_t$는 흘러들어온 그래디언트 $dy_t$에  $W_{hy}$를 곱한 값이다. $dh_{raw}$는 흘러들어온 그래디언트인 $dh_t$에 로컬 그래디언트인 $1-tanh^2_{h_raw}$를 곱해 구한다. 나머지도 동일한 방식으로 진행된다. 

RNN이 학습하는 parameter는 input $x$를 hidden layer $h$로 보내는 $W_xh$, 이전 hidden layer $h$에서 다음 hidden layer $h$로 보내는 $W_{hh}$, hidden layer $h$에서 output $ y$로 보내는 $W_{hy}$가 있다. 그리고 모든 시점의 state에서 이 paramter들은 동일하게 공유된다.

### **Disadvantages of RNNs**

RNN은 최근의 정보일수록 더 예측에 반영될 수 있게 설계되어 있다. 입력 정보와 입력 정보를 사용하려는 출력 지점의 거리가 멀 경우 역전파 할 시 기울기가 점차 줄어지든가 커져서 학습능력이 저하되기 때문이다. 즉, RNN은 Long-Term Dependencies가 없다는 단점이 있다.

이를 해결하기 위하여 먼저 기울기 한계점을 정해두어서, 만약에 한계점보다 더 크다면 크지 않도록 조정하는 방법이 있다. 그러나 이보다 효과적으로 RNN의 단점을 극복하기 위하여 새로운 모델이 등장했는데, 바로 LSTM 모델이다.

## Long Short-Term Memory Models(LSTM)

### Basic structure of LSTM

<p style="text-align: center;">
  <img src="/images/rnn_6.png" width="70%">
</p>

LSTM은 RNN의 hidden state에 cell-state를 추가한 구조이다. LSTM cell의 수식은 아래와 같다.

$$
\begin{align*}
f_t &= \sigma (W_{xh_f} x_t + W_{hh_f} h_{t-1} + b_{h_f}) \\
i_t &= \sigma (W_{xh_i} x_t + W_{hh_i} h_{t-1} + b_{h_i}) \\
o_t &= \sigma (W_{xh_o} x_t + W_{hh_o} h_{t-1} + b_{h_o}) \\
g_t &= \tanh (W_{xh_g} x_t + W_{hh_g} h_{t-1} + b_{h_g}) \\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t \\
h_t &= o_t \odot \tanh(c_t)
\end{align*}
$$

forget gate $f_t$는 과거 정보를 얼마나 잊을 것인지에 대해 결정하는 gate이다. $h_{t-1}$과 $x$를 받아 시그모이드를 취해준 값이 forget gate의 ouput이 된다. sigmoid 함수의 출력 범위는 0에서 1 사이이므로 0에 가까울수록 이전 상태의 정보를 잊게 되고 1에 가까울수록 이전 상태의 값을 기억하게 된다. 

input gate $i_t$는 현재 정보를 기억하기 위한 gate이다. $h_{t-1}$과 $x$를 받아 sigmoid를 취하고, 또 같은 입력으로 하이퍼볼릭 탄젠트를 취해준 다음 Hadamard product 연산을 한 값이 input gate가 내보내는 값이 된다. $ i_t \odot g_t $에서 $g$는 새로운 정보를 의미한다. -1~1 사이의 정보를 추출하는 것이며, 이것은 RNN에서 정보를 추출하는 것과 일반적으로 같다. 

따라서 $c_t = f_t \odot c_{t-1} + i_t \odot g_t$는 기존의 정보를 얼만큼 잊고, 새로운 정보로 얼만큼 대체할 것인지를 의미하며, $o_t \odot tanh(c_t)$는 계산된 최종 cell state를 얼마나 hidden state에 포함시켜줄 것인지에 대한 여부를 의미하게 된다.

### Forward pass of LSTM

<p style="text-align: center;">
  <img src="/images/rnn_7.png" width="80%">
</p>

위와 같이 LSTM의 Forward pass가 진행된다. 

### Backward pass of LSTM

<p style="text-align: center;">
  <img src="/images/rnn_8.png" width="80%">
</p>

$H_t$는 $i_t, f_t, o_t, g_t$로 구성된 행렬이므로, 각각에 해당하는 그래디언트를 합치면 $dH_t$를 만들 수 있다. $i, f, o$의 활성화 함수는 시그모이드이고, $g$만 tanh이므로 각각의 활성화 함수에 대한 로컬 그래디언트를 곱해주면 된다. 


### **Advantages of LSTM**

<p style="text-align: center;">
  <img src="/images/rnn_9.png" width="70%">
</p>

LSTM에서는 cell state가 flow하면서 3개의 gate(forget, input, output)가 점진적으로 추가되면서 변화되게 되고, 이 변화가 추가되는 부분은 Figure의 + 부분이다. LSTM에서는 + 부분이 역전파시 old state 방향으로 기울기를 그냥 전달하게 되므로 RNN의 역전파 부분에서 발생했던 기울기 소실 문제를 해결할 수 있다.












