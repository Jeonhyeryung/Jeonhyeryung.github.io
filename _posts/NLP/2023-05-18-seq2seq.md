---

title:  "[Paper Review] Sequence to Sequence Learning
with Neural Networks"

excerpt: "Seq2Seq 논문 리뷰"
categories:
  - NLP
toc_label: "  목차"
toc: true
toc_sticky: true

date: 2023-05-18
last_modified_at: 2023-05-18
---

## Background (Introduction)

Deep Neural Networks(DNNs)은 음성인식, visual object recognition과 같은 어려운 문제에서 좋은 성능을 보이는 훌륭한 기계학습 모델이다. DNNs이 강력한 이유는 적은 수의 단계에 대해 임의의 병렬 계산을 수행할 수 있기 때문이다. DNNs는 아주 복잡한 계산을 학습한다. 게다가 큰 DNNs은 labeld training set에 대하여 언제든지 오차 역전파를 이용하여 학습 할 수 있다. 또한 parameter setting을 통하여 DNN은 좋은 결과를 달성 할 수 있게 되고, 지도학습의 오차역전파는 이러한 paremeter을 찾아서 문제를 해결 할 수 있게 해준다.

DNN의 유연함과 강력함에도 불구하고 DNN은 오직 고정된 차원의 벡터를 input과 target encoder로 갖는 문제에만 적용 할 수 있다. 이건 매우 치명적인 한계이다. 왜냐하면 많은 중요한 문제들은 선험적으로 알지 못 하는 길이로 표현되기 때문이다. 예를 들어 음성인식이나 기계번역은 길이가 고정되지 않은 sequential problem이다. 그러므로, 도메인에 대해 독립적인 메소드가 필요한 시점이다.

<p style="text-align: center;">
  <img src="/images/seq2seq_1.png" width="100%">
</p>

본 논문에서는 위와 같은 한계점을 극복하기 위해 LSTM 아키텍처를 제시한다. LSTM은 sequence input을 사용하여 크고 고정된 차원의 벡터 표현을 얻는다. 그리고 또 다른 LSTM은 sequence representation vecoter로부터 output을 뽑아낸다는 것이다. 다음으로, LSTM은 input이 sequence라는 조건만을 제외하면 recurrent neural network language model이다. 따라서 LSTM은 시간에 의존적인 긴 데이터의 학습이 가능하고, 이러한 LSTM의 능력은 입력과 해당 출력 사이의 상당한 시간적인 차이를 가지고 있는 application을 해결하는데 도움을 준다.

## The model

The Recurrent Neural Network(RNN)은 연속적인 순전파 신경망의 자연스러운 형태이다. 주어진 input sequence $(X1, X2 ... Xt)$에 대하여 표준적인 RNN은 Output sequence $(Y1, Y2 ... Yr)$을 다음과 같은 수식을 반복하며 계산한다.

$$
h_t=sigm(W^{hx}x_t+W^{hh}h_{t-1}) \\
y_t = W^{yh}h_t
$$

RNN은 input과 output의 alignment가 주어지면 쉽게 sequence를 sequence로 mapping시킬 수 있다. 하지만 input과 output sequence가 일정한 규칙 없이 다양한 길이를 갖는 문제에 대해서는 분명하게 해당 방법을 적용시키기 어려워진다.

LSTM의 목표는 input sequence에 대한 output sequence에 대한 조건부 확률을 평가하는 것이다. LSTM은 우선 input sequence에 대하여 고정된 차원의 벡터 v를 연산하고, 아래 수식으로 연산되는 LSTM-LM의 확률을 계산한다.

$$
p(y_1,...,y_{T^\prime}|x_1,...,x_T) = \prod_{t=1}^{T^\prime} p(y_t|v,y_1,...,y_{t-1})
$$

본 논문의 실제 모델은 Figure1과 세 부분에서 다른 점을 가지고 있다. 첫째, 본 논문에서는 서로 다른 2개의 LSTM을 사용한다: 하나는 input sequence를 위한 것이고 또 다른 하나는 output sequence를 위한 것이다. 다른 모델을 쓰는 이유는 작은 계산 비용의 증가로 parameters의 수를 증가시킬 수 있다. 그리고 이것은 multiple laguage pairs을 자연스럽게 만들어준다. 둘째, 우리는 얕은 LSTM보다 깊은 층을 가지고 있는 LSTM이 좋은 성능을 보이는 것을 관찰해서, 4개의 Layer를 가지고 있는 LSTM을 이용하였다. 마지막으로 우리는 input sentence의 단어 순서를 거꾸로 넣었을 때 더 좋은 성능을 보이는 것을 관찰했다. 따라서 input의 순서를 거꾸로 넣어줌으로 성능을 향상시켰다.

## Experiments

### Dataset details

본 논문의 실험에서는 WMT'14 English to French dataset을 이용하였다. 일반적인 신경망 모델은 각 단어에 대한 vector 표현에 의존적이기 때문에 어휘를 선정하여 학습을 진행하였다. 영어에서 자주 사용되는 160,000개의 단어를 선정하였고, french에서는 자주 사용되는 80,000개의 단어를 선정하였다. 해당 단어에 포함되지 않은 단어는 <UNK> token으로 대체하였다.

### Decoding and Rescoring

아래와 같은 수식을 objective function으로 사용하여 log probability를 최대화 시키는 훈련을 진행하였다. $S$는 training set이고 $T$는 모델의 Translation 결과를 의미한다.

$$
1/|S| \sum_{(T,S)\in S} log \ p(T|S)
$$

훈련이 끝나고 나면 아래와 같은 식을 통하여 가장 가능성이 높은 번역을 찾아내었다. 본 논문의 실험에서는 left to right beam search decoder을 이용하여 가장 높은 확률의 번역을 찾아 내었다.

$$
\hat T=  \arg \max_{T}  \ (T|S)
$$

각각의 timestep마다 beam의 가설들을 확장해나가고 각 가설들은 모든 단어들이 가능하다. 이렇게 되면 가설의 크기가 굉장히 증가하기 때문에 model의 log확률에서 가장 높은 B개를 제외하고는 나머지 가설은 무시하였다. <EOS> symbol을 만날 때까지 가설의 크기는 커지게 된다. 빔 사이즈 1일 때 성과가 좋았으며 빔 사이즈 2는 빔 서치의 이점을 거의 다 제공한다.

본 논문에서는 baseline system에 의하여 만들어진 1000-best list에 대하여 rescoring을 진행하였다. 본 실험을 진행함에 있어서 LSTM에서 만들어지는 모든 가설에 대한 log probability를 이용했으며 baseling system의 점수와 LSTM의 점수의 평균을 이용했다.

### Reversing the Source Sentence

LSTM은 long term dependencies problem을 해결 할 수 있는 능력을 가지고 있지만, source sentence의 단어 순서를 거꾸로 입력하는 것이 더 좋은 성능을 보일 수 있음을 밝혀냈다. 이렇게 함으로, LSTM의 perplexity는 5.8에서 4.7로 감소했으며 BLEU scores는 25.9에서 30.6으로 증가하였다.

### Training details

1000개의 cell과 160,000개의 input 어휘, 80,000개의 output 어휘를 가지고 있는 1000차원의 word embedding을 가지고 있는 4layers LSTM을 이용하였다. 우리는 얕은 LSTM보다 깊은 LSTM이 더 좋은 성능을 보임을 알아냈고, 그 효과는 perplexity를 10% 가까이 줄일 수 있었다. output어휘 80,000개에 대한 navie softmax를 이용했다.

### Results

번역의 질을 평가하기 위하여 [BLEU score](https://pypi.org/project/bleu/)를 이용하였다. BLEU(bilingual Evaluation Understudy) score는 input과 output이 모두 sequence로 이루어져 있는 경우 사용하는 지표이며, 식은 아래와 같다.

$$
BLEU = \min (1, {output \ length\over reference \ length} )(\prod_{i=1}^4 precision_i)^{1\over4}
$$

<p style="text-align: center;">
  <img src="/images/seq2seq_2.png" width="100%">
</p>

<p style="text-align: center;">
  <img src="/images/seq2seq_3.png" width="100%">
</p>

실험 결과 제안한 방법을 앙상블 한 결과가 baseline 모델에 비해 더 좋은 성능을 보였으며, 또한 SMT모델의 상위 1000개 결과를 다시 제안한 방법으로 평가하여 각 모델의 평균을 결과로 하였을 때, SOTA 모델에 비해서는 약간 모자랐지만 더 향상된 성능을 얻을 수 있었다.









